\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The environment}{1}}
\newlabel{sec:environment}{{1.1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The state space representation}{1}}
\newlabel{sec:stateSpace}{{1.2}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Implementation details}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning algorithms}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}(M) Q-Learning}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}(M) Experiment with $\epsilon $ and optimistic initialization}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}(SC) Softmax action selection instead of $\epsilon $-greedy}{2}}
\newlabel{sec:softmax}{{2.3}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}(SC) On-policy Monte Carlo Control}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}(SC) Off-Policy Monte Carlo Control}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}(SC) Sarsa}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The $11 \times 11$ grid divided into eight symmetric pieces, with the corresponding possible moves which are also symmetric.\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:statespaceSymm}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Colormap of $V$-values, the brighter the color the higher the corresponding $V$-value. The prey is always located on the (1, 1) coordinate in this state representation.\relax }}{3}}
\newlabel{fig:NewStateRep}{{2}{3}}
\citation{*}
\bibstyle{plainnat}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {A}Class Diagram}{4}}
\newlabel{app:classDiagram}{{A}{4}}
