\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The environment}{1}}
\newlabel{sec:environment}{{1.1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The state space representation}{1}}
\newlabel{sec:stateSpace}{{1.2}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:statespaceSymm}{{1(a)}{2}}
\newlabel{sub@fig:statespaceSymm}{{(a)}{2}}
\newlabel{fig:NewStateRep}{{1(b)}{2}}
\newlabel{sub@fig:NewStateRep}{{(b)}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the symmetry and corresponding values of the new state space representation\relax }}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The $11 \times 11$ grid divided into eight symmetric pieces, with the corresponding possible moves which are also symmetric.}}}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Colormap of $V$-values, the brighter the color the higher the corresponding $V$-value. The prey is always located on the (1, 1) coordinate in this state representation.}}}{2}}
\newlabel{fig:statespaceIll}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Implementation details}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning algorithms}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}(M) Q-Learning}{2}}
\newlabel{sec:exercise1}{{2.1}{2}}
\@writefile{tdo}{\contentsline {todo}{Figure: {Plots on the performance of the agent over time for different $\alpha $ and for different $\gamma $ (discount factor) using $\epsilon $-greedy action selection.}}{3}}
\newlabel{plot:QEalphagamma}{{2.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}(M) Experiment with $\epsilon $ and optimistic initialization}{3}}
\newlabel{sec:exercise2}{{2.2}{3}}
\@writefile{tdo}{\contentsline {todo}{{Experiment with different values of $\epsilon $ and the optimistic initialization of the Q-table. Make up good values to test, and explain why you chose these values.}}{3}}
\pgfsyspdfmark {pgfid2}{20112834}{27617513}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}(SC) Softmax action selection instead of $\epsilon $-greedy}{3}}
\newlabel{sec:exercise3}{{2.3}{3}}
\@writefile{tdo}{\contentsline {todo}{Figure: {Plots on the performance of the agent over time for different $\alpha $ and for different $\gamma $ (discount factor) using softmax action selection.}}{3}}
\newlabel{plot:QSalphagamma}{{2.3}{3}}
\@writefile{tdo}{\contentsline {todo}{{Illustrate the difference between $\epsilon $-greedy and softmax, using graphs from your empirical results.}}{3}}
\pgfsyspdfmark {pgfid4}{20112834}{11427492}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Other ways to do learning}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}(SC) On-policy Monte Carlo Control}{4}}
\newlabel{sec:exercise4.1}{{2.4.1}{4}}
\@writefile{tdo}{\contentsline {todo}{{Explain the difference with other learning methods theoretically, and compare them using informative graphs.}}{4}}
\pgfsyspdfmark {pgfid5}{20112834}{37004541}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}(SC) Off-Policy Monte Carlo Control}{4}}
\newlabel{sec:exercise4.2}{{2.4.2}{4}}
\@writefile{tdo}{\contentsline {todo}{{Explain the difference with other learning methods theoretically, and compare them using informative graphs.}}{4}}
\pgfsyspdfmark {pgfid6}{20112834}{28240471}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}(SC) Sarsa}{4}}
\newlabel{sec:exercise4.3}{{2.4.3}{4}}
\@writefile{tdo}{\contentsline {todo}{{Explain the difference with other learning methods theoretically, and compare them using informative graphs.}}{4}}
\pgfsyspdfmark {pgfid8}{20112834}{16775641}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{4}}
\citation{*}
\bibstyle{plainnat}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {A}Class Diagram}{5}}
\newlabel{app:classDiagram}{{A}{5}}
