\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The environment}{1}}
\newlabel{environment}{{1.1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The state space representation}{1}}
\newlabel{stateSpace}{{1.2}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  The $11 \times 11$ grid divided into eight symmetric pieces, with the corresponding possible moves which are also symmetric.}}{2}}
\newlabel{statespaceSymm}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Colormap of the $V$-values resulting from Policy Evaluation for $\theta =0$ and $\gamma = 0.8$ \newline  using the ``efficient'' state space representation. The brighter the color the higher the corresponding $V$-value. The prey is always located on the (1, 1) coordinate in this state representation.}}{3}}
\newlabel{NewStateRep}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Implementation details}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning algorithms}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}(M) Q-Learning}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}(M) Experiment with $\epsilon $ and optimistic initialization}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}(SC) Softmax action selection instead of $\epsilon $-greedy}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}(SC) On-policy Monte Carlo Control}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}(SC) Off-Policy Monte Carlo Control}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}(SC) Sarsa}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{4}}
\citation{*}
\bibstyle{plainnat}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {A}Class Diagram}{5}}
\newlabel{app:classDiagram}{{A}{5}}
