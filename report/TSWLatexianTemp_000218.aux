\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The environment}{1}}
\newlabel{sec:environment}{{1.1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The state space representation}{1}}
\newlabel{sec:stateSpace}{{1.2}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:statespaceSymm}{{1(a)}{2}}
\newlabel{sub@fig:statespaceSymm}{{(a)}{2}}
\newlabel{fig:NewStateRep}{{1(b)}{2}}
\newlabel{sub@fig:NewStateRep}{{(b)}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the symmetry and corresponding values of the new state space representation\relax }}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The $11 \times 11$ grid divided into eight symmetric pieces, with the corresponding possible moves which are also symmetric.}}}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Colormap of $V$-values, the brighter the color the higher the corresponding $V$-value. The prey is always located on the (1, 1) coordinate in this state representation.}}}{2}}
\newlabel{fig:statespaceIll}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Implementation details}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning algorithms}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}(M) Q-Learning}{2}}
\newlabel{sec:exercise1}{{2.1}{2}}
\newlabel{fig:QLearningAlphaNRMSE}{{2(a)}{3}}
\newlabel{sub@fig:QLearningAlphaNRMSE}{{(a)}{3}}
\newlabel{fig:QLearningAlphaOA}{{2(b)}{3}}
\newlabel{sub@fig:QLearningAlphaOA}{{(b)}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plots on the performance of the agent over time for different values of the learning rate $\alpha $ using $\epsilon $-greedy action selection. Averaged over 100 runs with 1000 episodes each with $\epsilon = $ 0.1 and the discount factor $\gamma $ = 0.9.\relax }}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A comparison of the performance of the agent based on the resulting Normalized-Root-Mean-Squared-Error of the agent's state action pairs and the the estimated true state action pairs.}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {A comparison of the performance of the agent based on the resulting percentage of best actions in the state space action pairs $\displaystyle \qopname \relax m{max}_a Q(s_t,a_t)$ of the agent that are also present as the best actions in the estimated true state action pairs.}}}{3}}
\newlabel{plot:QLearningAlpha}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}(M) Experiment with $\epsilon $ and optimistic initialization}{3}}
\newlabel{sec:exercise2}{{2.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}(SC) Softmax action selection instead of $\epsilon $-greedy}{4}}
\newlabel{sec:exercise3}{{2.3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Other ways to do learning}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}(SC) On-policy Monte Carlo Control}{4}}
\newlabel{sec:exercise4.1}{{2.4.1}{4}}
\@writefile{tdo}{\contentsline {todo}{{Explain the difference with other learning methods theoretically, and compare them using informative graphs.}}{4}}
\pgfsyspdfmark {pgfid1}{20112834}{14172076}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}(SC) Off-Policy Monte Carlo Control}{4}}
\newlabel{sec:exercise4.2}{{2.4.2}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}(SC) Sarsa}{5}}
\newlabel{sec:exercise4.3}{{2.4.3}{5}}
\@writefile{tdo}{\contentsline {todo}{{Explain the difference with other learning methods theoretically, and compare them using informative graphs.}}{5}}
\pgfsyspdfmark {pgfid3}{20112834}{22897606}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{5}}
\citation{*}
\bibstyle{plainnat}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{\numberline {A}Class Diagram}{6}}
\newlabel{app:classDiagram}{{A}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Policy of On Line Monte Carlo}{6}}
\newlabel{policyOnLineMonteCarlo}{{B}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Policy of Off Line Monte Carlo}{6}}
\newlabel{policyOffLineMonteCarlo}{{C}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The policy resulting from On Line Monte Carlo learning method with $\tau $ is 0.9 and discount factor is 0.9 after 500 episodes\relax }}{7}}
\newlabel{policyOnline}{{1}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The estimation policy resulting from Off Line Monte Carlo learning method after 150 episodes of learning. The behavior policy that is used is generated by On Line Monte Carlo learning with $\tau $ is 0.9 and discount factor is 0.9 after 600 episodes\relax }}{8}}
\newlabel{policyOffline}{{2}{8}}
\newlabel{fig:QLearningGammaNRMSE}{{3(a)}{9}}
\newlabel{sub@fig:QLearningGammaNRMSE}{{(a)}{9}}
\newlabel{fig:QLearningGammaNRMSE_detail}{{3(b)}{9}}
\newlabel{sub@fig:QLearningGammaNRMSE_detail}{{(b)}{9}}
\newlabel{fig:QLearningGammaOA}{{3(c)}{9}}
\newlabel{sub@fig:QLearningGammaOA}{{(c)}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Plots on the performance of the agent over time for different values of the discount factor $\gamma $ using $\epsilon $-greedy action selection. Averaged over 100 runs with 1000 episodes each with $\epsilon = $ 0.1 and the learning rate $\alpha $ = 0.5.\relax }}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A comparison of the performance of the agent based on the resulting Normalized-Root-Mean-Squared-Error of the agent's state action pairs and the the estimated true state action pairs.}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {A closeup of the first 100 episodes of figure \G@refundefinedtrue\text {\normalfont \bfseries ??}\GenericWarning {               }{LaTeX Warning: Reference `fig:QLearningGammaNRMSE' on page 3 undefined}.}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {A comparison of the performance of the agent based on the resulting percentage of best actions in the state space action pairs of the agent that are also present as the best actions in the estimated true state action pairs.}}}{9}}
\newlabel{plot:QLearningGamma}{{3}{9}}
\newlabel{fig:secondMust_nrSteps}{{4(a)}{10}}
\newlabel{sub@fig:secondMust_nrSteps}{{(a)}{10}}
\newlabel{fig:secondMust_optimalAction}{{4(b)}{10}}
\newlabel{sub@fig:secondMust_optimalAction}{{(b)}{10}}
\newlabel{fig:secondMust_nrSteps_detail}{{4(c)}{10}}
\newlabel{sub@fig:secondMust_nrSteps_detail}{{(c)}{10}}
\newlabel{fig:secondMust_visitedPairs_detail}{{4(d)}{10}}
\newlabel{sub@fig:secondMust_visitedPairs_detail}{{(d)}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plots on the performance of the agent over time for different values of $\epsilon $ and initialization values using $\epsilon $-greedy action selection. Averaged over 100 runs with 1000 episodes each with $\gamma = $ 0.9 and the learning rate $\alpha $ = 0.5.\relax }}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A comparison of the performance of the agent based on the average number of steps taken for each episode.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {A comparison of the performance of the agent based on the resulting percentage of best actions in the state space action pairs of the agent that are also present as the best actions in the estimated true state action pairs.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {A closeup of the first 20 episodes of figure \G@refundefinedtrue\text {\normalfont \bfseries ??}\GenericWarning {               }{LaTeX Warning: Reference `fig:secondMust_nrSteps' on page 3 undefined}.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {A comparison of the performance of the agent based on the percentage of state action pairs visited after each episode.}}}{10}}
\newlabel{plot:secondMust}{{4}{10}}
\newlabel{fig:secondShould_Greedy_nrSteps}{{5(a)}{11}}
\newlabel{sub@fig:secondShould_Greedy_nrSteps}{{(a)}{11}}
\newlabel{fig:secondShould_Softmax_nrSteps}{{5(b)}{11}}
\newlabel{sub@fig:secondShould_Softmax_nrSteps}{{(b)}{11}}
\newlabel{fig:secondShould_Greedy_OA}{{5(c)}{11}}
\newlabel{sub@fig:secondShould_Greedy_OA}{{(c)}{11}}
\newlabel{fig:secondShould_Softmax_OA}{{5(d)}{11}}
\newlabel{sub@fig:secondShould_Softmax_OA}{{(d)}{11}}
\newlabel{fig:secondShould_Greedy_visitedPairs}{{5(e)}{11}}
\newlabel{sub@fig:secondShould_Greedy_visitedPairs}{{(e)}{11}}
\newlabel{fig:secondShould_Softmax_visitedPairs}{{5(f)}{11}}
\newlabel{sub@fig:secondShould_Softmax_visitedPairs}{{(f)}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Plots on the performance of the agent over time for different values of $\epsilon $ using $\epsilon $-greedy action selection (figures \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Greedy_nrSteps' on page 11 undefined}, \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Greedy_OA' on page 11 undefined} and \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Greedy_visitedPairs' on page 11 undefined}) and for different values of $\tau $ using the Softmax action selection (figures \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Softmax_nrSteps' on page 11 undefined}, \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Softmax_OA' on page 11 undefined} and \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Softmax_visitedPairs' on page 11 undefined}). Averaged over 100 runs with 1000 episodes each with $\gamma = $ 0.9, the learning rate $\alpha $ = 0.5 and the initialization value at 15.\relax }}{11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A comparison of the performance of the agent based on the average number of steps taken for each episode.}}}{11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {A comparison of the performance of the agent based on the average number of steps taken for each episode.}}}{11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {A comparison of the performance of the agent based on the resulting percentage of best actions in the state space action pairs of the agent that are also present as the best actions in the estimated true state action pairs.}}}{11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {A comparison of the performance of the agent based on the resulting percentage of best actions in the state space action pairs of the agent that are also present as the best actions in the estimated true state action pairs.}}}{11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {A comparison of the performance of the agent based on the percentage of state action pairs visited after each episode.}}}{11}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {A comparison of the performance of the agent based on the percentage of state action pairs visited after each episode.}}}{11}}
\newlabel{plot:secondShould}{{5}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Number of steps for each episode as the agent learns for different values of the discount factor. The values are averaged over 50 trials where the agent learns for 100 runs. The value of $\tau $ is set to 0.9\relax }}{12}}
\newlabel{fig:onLineMonteCarloDiscount}{{6}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Number of steps for each episode as the agent learns for different values of $\tau $. The values are averaged over 50 trials where the agent learns for 100 runs. The value of the discount factor is set to 0.8\relax }}{12}}
\newlabel{fig:onLineMonteCarloTau}{{7}{12}}
\newlabel{offLine}{{\caption@xref {offLine}{ on input line 219}}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The number of steps for each episode averaged over 5 trials where the agent learns for 150 runs\relax }}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Class diagram}{14}}
\newlabel{app:classDiagram}{{D}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  A class diagram of our code. For clarity purposes, not all methods and attributes of the classes are shown.\relax }}{14}}
\newlabel{pic:classDiagram}{{9}{14}}
