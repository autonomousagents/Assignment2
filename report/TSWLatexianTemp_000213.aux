\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The environment}{1}}
\newlabel{sec:environment}{{1.1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The state space representation}{1}}
\newlabel{sec:stateSpace}{{1.2}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:statespaceSymm}{{1(a)}{2}}
\newlabel{sub@fig:statespaceSymm}{{(a)}{2}}
\newlabel{fig:NewStateRep}{{1(b)}{2}}
\newlabel{sub@fig:NewStateRep}{{(b)}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the symmetry and corresponding values of the new state space representation\relax }}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {The $11 \times 11$ grid divided into eight symmetric pieces, with the corresponding possible moves which are also symmetric.}}}{2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Colormap of $V$-values, the brighter the color the higher the corresponding $V$-value. The prey is always located on the (1, 1) coordinate in this state representation.}}}{2}}
\newlabel{fig:statespaceIll}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Implementation details}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning algorithms}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}(M) Q-Learning}{2}}
\newlabel{sec:exercise1}{{2.1}{2}}
\newlabel{fig:QLearningAlphaNRMSE}{{2(a)}{3}}
\newlabel{sub@fig:QLearningAlphaNRMSE}{{(a)}{3}}
\newlabel{fig:QLearningAlphaOA}{{2(b)}{3}}
\newlabel{sub@fig:QLearningAlphaOA}{{(b)}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plots on the performance of the agent over time for different values of the learning rate $\alpha $ using $\epsilon $-greedy action selection. Averaged over 100 runs with 1000 episodes each with $\epsilon = $ 0.1 and the discount factor $\gamma $ = 0.9.\relax }}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A comparison of the performance of the agent based on the resulting Normalized-Root-Mean-Squared-Error of the agent's state action pairs and the the estimated true state action pairs.}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {A comparison of the performance of the agent based on the resulting percentage of best actions in the state space action pairs $\displaystyle \qopname \relax m{max}_a Q(s_t,a_t)$ of the agent that are also present as the best actions in the estimated true state action pairs.}}}{3}}
\newlabel{plot:QLearningAlpha}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}(M) Experiment with $\epsilon $ and optimistic initialization}{3}}
\newlabel{sec:exercise2}{{2.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}(SC) Softmax action selection instead of $\epsilon $-greedy}{4}}
\newlabel{sec:exercise3}{{2.3}{4}}
\@writefile{tdo}{\contentsline {todo}{{Experiment with different values of $\epsilon $ and the optimistic initialization of the Q-table. Make up good values to test, and explain why you chose these values.}}{4}}
\pgfsyspdfmark {pgfid1}{20112834}{37263953}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}(SC) Softmax action selection instead of $\epsilon $-greedy}{4}}
\newlabel{sec:exercise3}{{2.4}{4}}
\@writefile{tdo}{\contentsline {todo}{Figure: {Plots on the performance of the agent over time for different $\alpha $ and for different $\gamma $ (discount factor) using softmax action selection.}}{4}}
\newlabel{plot:QSalphagamma}{{2.4}{4}}
\@writefile{tdo}{\contentsline {todo}{{Illustrate the difference between $\epsilon $-greedy and softmax, using graphs from your empirical results.}}{4}}
\pgfsyspdfmark {pgfid3}{20112834}{21073932}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Other ways to do learning}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}(SC) On-policy Monte Carlo Control}{4}}
\newlabel{sec:exercise4.1}{{2.5.1}{4}}
\@writefile{tdo}{\contentsline {todo}{{Explain the difference with other learning methods theoretically, and compare them using informative graphs.}}{5}}
\pgfsyspdfmark {pgfid4}{20112834}{36937182}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.2}(SC) Off-Policy Monte Carlo Control}{5}}
\newlabel{sec:exercise4.2}{{2.5.2}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.3}(SC) Sarsa}{5}}
\newlabel{sec:exercise4.3}{{2.5.3}{5}}
\@writefile{tdo}{\contentsline {todo}{{Explain the difference with other learning methods theoretically, and compare them using informative graphs.}}{5}}
\pgfsyspdfmark {pgfid6}{20112834}{7714649}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{6}}
\citation{*}
\bibstyle{plainnat}
\bibdata{references}
\newlabel{fig:QLearningGammaNRMSE}{{3(a)}{8}}
\newlabel{sub@fig:QLearningGammaNRMSE}{{(a)}{8}}
\newlabel{fig:QLearningGammaNRMSE_detail}{{3(b)}{8}}
\newlabel{sub@fig:QLearningGammaNRMSE_detail}{{(b)}{8}}
\newlabel{fig:QLearningGammaOA}{{3(c)}{8}}
\newlabel{sub@fig:QLearningGammaOA}{{(c)}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Plots on the performance of the agent over time for different values of the discount factor $\gamma $ using $\epsilon $-greedy action selection. Averaged over 100 runs with 1000 episodes each with $\epsilon = $ 0.1 and the learning rate $\alpha $ = 0.5.\relax }}{8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A comparison of the performance of the agent based on the resulting Normalized-Root-Mean-Squared-Error of the agent's state action pairs and the the estimated true state action pairs.}}}{8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {A closeup of the first 100 episodes of figure \G@refundefinedtrue\text {\normalfont \bfseries ??}\GenericWarning {               }{LaTeX Warning: Reference `fig:QLearningGammaNRMSE' on page 3 undefined}.}}}{8}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {A comparison of the performance of the agent based on the resulting percentage of best actions in the state space action pairs of the agent that are also present as the best actions in the estimated true state action pairs.}}}{8}}
\newlabel{plot:QLearningGamma}{{3}{8}}
\newlabel{fig:secondMust_nrSteps}{{4(a)}{9}}
\newlabel{sub@fig:secondMust_nrSteps}{{(a)}{9}}
\newlabel{fig:secondMust_optimalAction}{{4(b)}{9}}
\newlabel{sub@fig:secondMust_optimalAction}{{(b)}{9}}
\newlabel{fig:secondMust_nrSteps_detail}{{4(c)}{9}}
\newlabel{sub@fig:secondMust_nrSteps_detail}{{(c)}{9}}
\newlabel{fig:secondMust_visitedPairs_detail}{{4(d)}{9}}
\newlabel{sub@fig:secondMust_visitedPairs_detail}{{(d)}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plots on the performance of the agent over time for different values of $\epsilon $ and initialization values using $\epsilon $-greedy action selection. Averaged over 100 runs with 1000 episodes each with $\gamma = $ 0.9 and the learning rate $\alpha $ = 0.5.\relax }}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A comparison of the performance of the agent based on the average number of steps taken for each episode.}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {A comparison of the performance of the agent based on the resulting percentage of best actions in the state space action pairs of the agent that are also present as the best actions in the estimated true state action pairs.}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {A closeup of the first 20 episodes of figure \G@refundefinedtrue\text {\normalfont \bfseries ??}\GenericWarning {               }{LaTeX Warning: Reference `fig:secondMust_nrSteps' on page 3 undefined}.}}}{9}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {A comparison of the performance of the agent based on the percentage of state action pairs visited after each episode.}}}{9}}
\newlabel{plot:secondMust}{{4}{9}}
\newlabel{fig:secondShould_Greedy_nrSteps}{{5(a)}{10}}
\newlabel{sub@fig:secondShould_Greedy_nrSteps}{{(a)}{10}}
\newlabel{fig:secondShould_Softmax_nrSteps}{{5(b)}{10}}
\newlabel{sub@fig:secondShould_Softmax_nrSteps}{{(b)}{10}}
\newlabel{fig:secondShould_Greedy_OA}{{5(c)}{10}}
\newlabel{sub@fig:secondShould_Greedy_OA}{{(c)}{10}}
\newlabel{fig:secondShould_Softmax_OA}{{5(d)}{10}}
\newlabel{sub@fig:secondShould_Softmax_OA}{{(d)}{10}}
\newlabel{fig:secondShould_Greedy_visitedPairs}{{5(e)}{10}}
\newlabel{sub@fig:secondShould_Greedy_visitedPairs}{{(e)}{10}}
\newlabel{fig:secondShould_Softmax_visitedPairs}{{5(f)}{10}}
\newlabel{sub@fig:secondShould_Softmax_visitedPairs}{{(f)}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Plots on the performance of the agent over time for different values of $\epsilon $ using $\epsilon $-greedy action selection (figures \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Greedy_nrSteps' on page 10 undefined}, \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Greedy_OA' on page 10 undefined} and \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Greedy_visitedPairs' on page 10 undefined}) and for different values of $\tau $ using the Softmax action selection (figures \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Softmax_nrSteps' on page 10 undefined}, \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Softmax_OA' on page 10 undefined} and \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `fig:secondShould_Softmax_visitedPairs' on page 10 undefined}). Averaged over 100 runs with 1000 episodes each with $\gamma = $ 0.9, the learning rate $\alpha $ = 0.5 and the initialization value at 15.\relax }}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {A comparison of the performance of the agent based on the average number of steps taken for each episode.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {A comparison of the performance of the agent based on the average number of steps taken for each episode.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {A comparison of the performance of the agent based on the resulting percentage of best actions in the state space action pairs of the agent that are also present as the best actions in the estimated true state action pairs.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {A comparison of the performance of the agent based on the resulting percentage of best actions in the state space action pairs of the agent that are also present as the best actions in the estimated true state action pairs.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {A comparison of the performance of the agent based on the percentage of state action pairs visited after each episode.}}}{10}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {A comparison of the performance of the agent based on the percentage of state action pairs visited after each episode.}}}{10}}
\newlabel{plot:secondShould}{{5}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Number of steps for each episode as the agent learns for different values of the discount factor. The values are averaged over 50 trials where the agent learns for 100 runs. The value of $\tau $ is set to 0.9\relax }}{11}}
\newlabel{fig:onLineMonteCarloDiscount}{{6}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Number of steps for each episode as the agent learns for different values of $\tau $. The values are averaged over 50 trials where the agent learns for 100 runs. The value of the discount factor is set to 0.8\relax }}{11}}
\newlabel{fig:onLineMonteCarloTau}{{7}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Class diagram}{12}}
\newlabel{app:classDiagram}{{A}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  A class diagram of our code. For clarity purposes, not all methods and attributes of the classes are shown.\relax }}{12}}
\newlabel{pic:classDiagram}{{8}{12}}
